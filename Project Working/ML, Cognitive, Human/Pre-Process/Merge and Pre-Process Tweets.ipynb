{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'Dataset Generator.ipynb', 'dataset.xlsx', 'latest_dataset.xlsx', 'Pre-Process', 'queries.xlsx']\n",
      "C:\\Users\\User\\Desktop\\Chatbot\\Project Working\\ML, Cognitive, Human\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "print(os.listdir())\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('latest_dataset.xlsx',usecols = \"A,B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiliaze attributes\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set (stopwords.words('english'))\n",
    "tok = WordPunctTokenizer()\n",
    "queries = []\n",
    "\n",
    "for query in df['query']:\n",
    "    #remove all links\n",
    "    data = re.sub(r\"@\\S*|#|http\\S+\", \"\", query)\n",
    "    #then apply lemmatization\n",
    "    data1 = data.split()\n",
    "    newdata = \"\"\n",
    "    for data in data1:\n",
    "        datas = lemmatizer.lemmatize(data)\n",
    "        newdata = newdata + \" \" + datas\n",
    "\n",
    "    #then finally check for stopwords\n",
    "    words_token = word_tokenize(newdata)\n",
    "    #filtered_sentence = [w for w in words_token if not w in stop_words]\n",
    "    filtered_sentence = \"\"\n",
    "    for w in words_token:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence = filtered_sentence + \" \" + w\n",
    "\n",
    "    queries.append(filtered_sentence)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "#definig regular expressions\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "#creating a function which does all the work\n",
    "def query_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    #only letters remove other characters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "#running the function and returns the result in dataframe\n",
    "queries_new = []\n",
    "for query in queries:\n",
    "    queries_new.append(query_cleaner(query))\n",
    "\n",
    "#append it to the dataframe\n",
    "df['query'] = queries_new\n",
    "df.dropna(inplace=True, how='any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('pre-process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Desktop\\\\Chatbot\\\\Project Working\\\\ML, Cognitive, Human\\\\pre-process'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output dataframe to excel format\n",
    "df.to_excel('cleaned_queries.xlsx',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
